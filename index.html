<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">

  <meta property="og:title" content="Graph Collaborative Attention Network for Link Prediction in Knowledge Graphs" />
  <meta property="og:url" content="" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />
  <!-- <meta name="google-site-verification" content="c-OO9s8QGRGOjdJTz3dyfcYlgskfbLXnwRRAzQLcoWw" /> -->

  <title>Graph Collaborative Attention Network for Link Prediction in Knowledge Graphs</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-62B92XNTBK"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-62B92XNTBK');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="icon" href="static/figures/icon2.png">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js"
    onload="renderMathInElement(document.body);"></script>
</head>

<body>


  <section class="publication-header">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <!-- <div class="columns is-centered"> -->
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Graph Collaborative Attention Network for Link Prediction in
            Knowledge Graphs</h1>
          <!-- <div class="is-size-3 publication-authors">
            ACM ICMI 2024
          </div> -->
        </div>
      </div>
    </div>

  </section>

  <section class="publication-author-block">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <div class="is-size-5 publication-authors">
              <span class="author-block"><a href="https://orcid.org/0009-0007-0898-5923" target="_blank">Thanh
                  Hoang-Minh</a></span>
              <!-- <span class="author-block"><a href="https://en.hcmus.edu.vn/profile/assoc-prof-dr-ly-quoc-ngoc/"
                  target="_blank">Ngoc Ly-Quoc</a></span> -->
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Department of Computer Science, VNUHCM - University of Science</span>
              <span class="author-block">Ho Chi Minh City, Vietnam</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2507.03947" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://github.com/hmthanh/GCAT" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://opendatalab.com/OpenDataLab/WN18RR"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <svg class="svg-inline--fa fa-w-16" fill="white" width="25" height="18.52"
                        viewBox="0 0 95.103 95.103">
                        <g>
                          <g>
                            <g>
                              <path
                                d="M47.561,0C25.928,0,8.39,6.393,8.39,14.283v11.72c0,7.891,17.538,14.282,39.171,14.282       c21.632,0,39.17-6.392,39.17-14.282v-11.72C86.731,6.393,69.193,0,47.561,0z">
                              </path>
                            </g>
                          </g>
                          <g>
                            <g>
                              <path
                                d="M47.561,47.115c-20.654,0-37.682-5.832-39.171-13.227c-0.071,0.353,0,19.355,0,19.355       c0,7.892,17.538,14.283,39.171,14.283c21.632,0,39.17-6.393,39.17-14.283c0,0,0.044-19.003-0.026-19.355       C85.214,41.284,68.214,47.115,47.561,47.115z">
                              </path>
                            </g>
                          </g>
                          <path
                            d="M86.694,61.464c-1.488,7.391-18.479,13.226-39.133,13.226S9.875,68.854,8.386,61.464L8.39,80.82     c0,7.891,17.538,14.282,39.171,14.282c21.632,0,39.17-6.393,39.17-14.282L86.694,61.464z">
                          </path>
                        </g>
                      </svg>
                    </span>
                    <span>WN18RR</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://service.tib.eu/ldmservice/dataset/fb15k-237-benchmark"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <svg class="svg-inline--fa fa-w-16" fill="white" width="25" height="18.52"
                        viewBox="0 0 95.103 95.103">
                        <g>
                          <g>
                            <g>
                              <path
                                d="M47.561,0C25.928,0,8.39,6.393,8.39,14.283v11.72c0,7.891,17.538,14.282,39.171,14.282       c21.632,0,39.17-6.392,39.17-14.282v-11.72C86.731,6.393,69.193,0,47.561,0z">
                              </path>
                            </g>
                          </g>
                          <g>
                            <g>
                              <path
                                d="M47.561,47.115c-20.654,0-37.682-5.832-39.171-13.227c-0.071,0.353,0,19.355,0,19.355       c0,7.892,17.538,14.283,39.171,14.283c21.632,0,39.17-6.393,39.17-14.283c0,0,0.044-19.003-0.026-19.355       C85.214,41.284,68.214,47.115,47.561,47.115z">
                              </path>
                            </g>
                          </g>
                          <path
                            d="M86.694,61.464c-1.488,7.391-18.479,13.226-39.133,13.226S9.875,68.854,8.386,61.464L8.39,80.82     c0,7.891,17.538,14.282,39.171,14.282c21.632,0,39.17-6.393,39.17-14.282L86.694,61.464z">
                          </path>
                        </g>
                      </svg>
                    </span>
                    <span>FB15k</span>
                  </a>
                </span>


                <!-- </span> -->
                <!-- Colab Link. -->
              </div>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">

        <div class="column is-centered has-text-centered">
          <img src="static/figures/GCAT.png" alt="cars peace" />
        </div>

        <h2 class="subtitle has-text-centered">
          </span> Illustration of the encoder layers in the GCAT model.
        </h2>
      </div>
    </div>
  </section>

  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">

          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <!-- <p>
              Knowledge graphs offer a structured representation of real-world entities and their relationships,
              enabling a wide range of applications from information retrieval to automated reasoning. In this paper, we
              conduct a systematic comparison between traditional rule-based approaches and modern deep learning methods
              for link prediction. We focus on KBGAT, a graph neural network model that leverages multi-head attention
              to jointly encode both entity and relation features within local neighborhood structures. To advance this
              line of research, we introduce \textbf{GCAT} (Graph Collaborative Attention Network), a refined model that
              enhances context aggregation and interaction between heterogeneous nodes. Experimental results on four
              widely-used benchmark datasets demonstrate that GCAT not only consistently outperforms rule-based methods
              but also achieves competitive or superior performance compared to existing neural embedding models. Our
              findings highlight the advantages of attention-based architectures in capturing complex relational
              patterns for knowledge graph completion tasks. -->
            <p>
              Knowledge graphs offer a structured representation of real-world entities and their relationships,
              enabling a wide range of applications from information retrieval to automated reasoning. In this paper, we
              conduct a systematic comparison between traditional rule-based approaches and modern deep learning methods
              for link prediction. We focus on <strong>KBGAT</strong>, a graph neural network model that leverages
              multi-head attention to jointly encode both entity and relation features within local neighborhood
              structures.
            </p>
            <p>
              To advance this line of research, we introduce <strong>GCAT</strong> (Graph Collaborative Attention
              Network), a refined model that enhances context aggregation and interaction between heterogeneous nodes.
              Experimental results on four widely-used benchmark datasets demonstrate that GCAT not only consistently
              outperforms rule-based methods but also achieves competitive or superior performance compared to existing
              neural embedding models. Our findings highlight the advantages of attention-based architectures in
              capturing complex relational patterns for knowledge graph completion tasks.
            </p>

          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>



  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">

        <div class="column is-centered has-text-centered">
          <img src="static/figures/KnowledgeGraphSample.png" alt="Knowledge Graph Sample" width="1000" />
        </div>
      </div>
      <h2 class="subtitle has-text-centered">
        </span>Sample data of knowledge graph.
      </h2>
    </div>
    </div>
  </section>


  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <!--         <h2 class="title is-3">How does it work?</h2> -->
          <div class="content has-text-justified">
            <section id="graph-embedding">
              <!-- <h2>Graph Embedding Problem Settings</h2> -->

              <p>
                Graph embedding methods differ based on the input graph type:
                homogeneous, heterogeneous, with auxiliary info, or constructed from non-relational data.
                While input is dataset-defined, the output is task-specific.
              </p>

              <p>
                The main embedding output types include:
                <strong>Node Embedding</strong> (individual node vectors for node-level tasks),
                <strong>Edge Embedding</strong> (vectors for node pairs or relations),
                <strong>Hybrid Embedding</strong> (combinations like substructures), and
                <strong>Whole-Graph Embedding</strong> (one vector per graph for tasks like classification).
              </p>

              <p>
                Node embeddings preserve neighborhood similarity (e.g., first-/second-order proximity).
                Edge embeddings are crucial in knowledge graphs or link prediction.
                Hybrid embeddings integrate multiple structural levels.
                Whole-graph embeddings require hierarchical aggregation and are suited for small graphs (e.g.,
                molecules).
              </p>

              <div class="column is-centered has-text-centered">
                <img src="static/figures/EmbeddingMethods.png" alt="Embedding Methods" width="1000" />
              </div>
            </section>


          </div>
          <!-- <div class="content has-text-justified">
            <p>
              We conduct an extensive set of experiments
              to evaluate our framework. Our system outperforms all baselines
              both qualitatively and quantitatively, as evidenced by FGD, SRGR,
              SC, and SRA metrics, and user study results.
          </div> -->
        </div>
        <!--/ Abstract. -->
      </div>
  </section>

  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">

          <div class="content has-text-justified">
            <section id="sec:GAT">
              <h2 class="title is-3">Graph Attention Network</h2>
              <p>
                GAT introduces masked multi-head self-attention to graphs by allowing each node to attend only to its
                immediate neighbors. This avoids unnecessary computation and preserves graph structure.
              </p>

              <p>
                Given initial entity embeddings
                <span class="katex">\( \mathbf{E} = \{\overrightarrow{e_1}, ..., \overrightarrow{e_{N_e}} \} \)</span>,
                the model linearly transforms them and computes attention scores only among first-order neighbors:
              </p>

              <div class="katex-block">
                \[
                e_{ij} = \text{LeakyReLU} \left( \vec{a}^T [\mathbf{W} \overrightarrow{e_i} || \mathbf{W}
                \overrightarrow{e_j}] \right)
                \]
              </div>

              <p>
                These raw scores are normalized with softmax:
              </p>

              <div class="katex-block">
                \[
                \alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k \in \mathcal{N}_i} \exp(e_{ik})}
                \]
              </div>

              <p>
                Finally, embeddings are updated by aggregating neighbors’ representations:
              </p>

              <div class="katex-block">
                \[
                \overrightarrow{e'_i} = \sigma\left( \sum_{j \in \mathcal{N}_i} \alpha_{ij} \mathbf{W}
                \overrightarrow{e_j} \right)
                \]
              </div>

              <p>
                Multi-head attention further improves stability by either concatenating or averaging the outputs of
                multiple attention heads. GAT is especially effective for graph-structured data without requiring matrix
                inversion or dense adjacency.
              </p>
            </section>

          </div>
          <div>

            <section id="kbgat-model">
              <h2 class="title is-3">KBGAT Model</h2>

              <p>
                In knowledge graphs, entities often play multiple roles depending on their relations. To capture this
                context, the <strong>KBGAT</strong> model extends GAT by integrating both
                relation
                and neighboring features into the attention mechanism.
              </p>

              <p>
                KBGAT follows an <em>encoder-decoder</em> framework:
                entity embeddings are initialized using <strong>TransE</strong>,
                enhanced via <strong>GAT-based encoder</strong>,
                and scored through a <strong>ConvKB decoder</strong>:
              </p>

              <div class="katex-block">
                \[
                \text{entity} \xrightarrow{\text{TransE}} \vec{e}_{\text{TransE}} \xrightarrow{\text{GAT}}
                \vec{e}_{\text{KBGAT}} \xrightarrow{\text{ConvKB}} \text{score}
                \]
              </div>

              <p>
                The initial embeddings are learned using the TransE principle:
              </p>

              <div class="katex-block">
                \[
                \vec{h} + \vec{r} \approx \vec{t}
                \]
              </div>

              <p>
                where <span class="katex">\( \vec{h} \)</span> and <span class="katex">\( \vec{t} \)</span> are the head
                and tail entity embeddings, and <span class="katex">\( \vec{r} \)</span> is the relation vector. This
                forms the basis for encoding semantic consistency across triples.
              </p>
            </section>

            <section id="kbgat-encoder-decoder">
              <h2>KBGAT Encoder and ConvKB Decoder</h2>

              <p>
                The <strong>KBGAT encoder</strong> extends GAT by combining entity and relation features.
                It transforms initial embeddings using multi-head attention across n-hop neighbors and relation paths:
              </p>

              <div class="katex-block">
                \[
                \overrightarrow{t_{ijk}} = \mathbf{W}_1 [\vec{e}_i || \vec{e}_j || \vec{r}_k], \quad
                \alpha_{ijk} = \text{softmax}_{jk}(\text{LeakyReLU}(\mathbf{W}_2 \vec{t}_{ijk}))
                \]
              </div>

              <p>
                These attention-weighted triple vectors are aggregated to update entity embeddings.
                A residual connection preserves initial embeddings:
              </p>

              <div class="katex-block">
                \[
                \mathbf{H} = \mathbf{W}^E \mathbf{E} + \mathbf{E''}
                \]
              </div>

              <p>
                In the decoder, <strong>ConvKB</strong> uses convolutional filters to extract patterns from each triple:
              </p>

              \[
              f(t_{ijk}) = \left( \mathop{\Big\|}_{m=1}^{\Omega} \mathrm{ReLU}\left([\vec{e}_i, \vec{r}_k, \vec{e}_j] *
              \omega^m\right) \right) \cdot \mathbf{W}
              \]

              <p>
                The model is trained using a soft-margin loss function:
              </p>

              \[
              \mathcal{L} = \sum_{t^k_{ij} \in \{S \cup S'\}} \log\left(1 + \exp\left(l_{t^k_{ij}} \cdot
              f(t^k_{ij})\right)\right) + \frac{\lambda}{2} \left\lVert \mathbf{W} \right\rVert_2^2
              \]

              where the label \( l_{t^k_{ij}} \) is defined as:

              \[
              l_{t^k_{ij}} =
              \begin{cases}
              1 & \text{if } t^k_{ij} \in S \\
              -1 & \text{if } t^k_{ij} \in S'
              \end{cases}
              \]

              The final output of the ConvKB model is the ranking score \( f(t^k_{ij}) \) for each triple prediction.

            </section>

          </div>

        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    </div>
  </section>


  <section class="hero is-small">
    <div class="hero-body">
      <div class="column is-centered has-text-centered">
        <h3 class="title is-4">Baseline TransE Embedding</h3>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="column is-centered has-text-centered">
            <!-- <p><b>Text Prompt: “the person is <font color="red">angry</font>.”</b></p> -->
            <img src="static/figures/transE/TransE_Embedding.png" alt="TransE Embedding" width="900" />
            <!-- <video poster="" id="tree" width=300>
              <source src="static/figures/text_prompt/angry.mp4" type="video/mp4">
            </video> -->
          </div>
          <div class="column is-centered has-text-centered">
            <!-- <p><b> Text Prompt: “standing like a <font color="red">boxer</font>.”</b></p> -->

            <!-- <video poster="" id="tree" controls width=300>
              <source src="static/figures/text_prompt/boxer.mp4" type="video/mp4">
            </video> -->
            <img src="static/figures/transE/algorithm.png" alt="algorithm" width="500" />
          </div>
          <div class="column is-centered has-text-centered">
            <p>Illustration of embedding vectors in the TransE model</p>

            <img src="static/figures/TranE.png" alt="TranE" width="650" />
          </div>
          <!-- <div class="column is-centered has-text-centered">
            <p><b> Text Prompt: “the person is <font color="red">happy</font> and <font color="red">excited</font>.”</b>
            </p>

            <video poster="" id="tree" controls width=300>
              <source src="static/figures/text_prompt/happy.mp4" type="video/mp4">
            </video>
          </div>
          <div class="column is-centered has-text-centered">
            <p><b> Text Prompt: “the person is <font color="red">sad</font>.”</b></p>

            <video poster="" id="tree" controls width=300>
              <source src="static/figures/text_prompt/sad.mp4" type="video/mp4">
            </video>
          </div>
          <div class="column is-centered has-text-centered">
            <p><b> Text Prompt: “a person is <font color="red">holding a cup of coffee in the right hand</font>.”</b>
            </p>

            <video poster="" id="tree" controls width=300>
              <source src="static/figures/text_prompt/hold.mp4" type="video/mp4">
            </video>
          </div>
          <div class="column is-centered has-text-centered">
            <p><b> Text Prompt: “the person is <font color="red">playing the guitar</font>.”</b></p>

            <video poster="" id="tree" controls width=300>
              <source src="static/figures/text_prompt/guitar.mp4" type="video/mp4">
            </video>
          </div> -->

        </div>
      </div>
  </section>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="column is-centered has-text-centered">
        <h3 class="title is-4">Experiments on Wordnet 18 and FB15k Dataset</h3>
        <!-- <p><b> (The left video is the motion prompt, and the right video shows the results.)</b></p> -->

        <div id="results-carousel" class="carousel results-carousel">
          <div class="column is-centered has-text-centered">
            <img src="static/figures/dataset.png" alt="dataset" width="600" />
          </div>

          <div class="column is-centered has-text-centered">
            <img src="static/figures/WNStats.png" alt="WN Stats" width="1000" />
          </div>

          <div class="column is-centered has-text-centered">
            <img src="static/figures/WN18/WN18.png" alt="WN18" width="900" />

            <!-- <video poster="" id="tree" muted controls width=250>
              <source src="static/figures/motion_prompt/motion_prompt.mp4" type="video/mp4">
            </video> -->
            <!-- <video poster="" id="tree" controls width=300>
              <source src="static/figures/motion_prompt/neutral.mp4" type="video/mp4">
            </video>
            <p><b>Motion Prompt: “<font color="red">high right hand</font> and <font color="red">low left hand</font>
                .”</b></p>
            </br> -->
          </div>

          <div class="column is-centered has-text-centered">
            <!-- <video poster="" id="tree" muted controls width=250>
              <source src="static/figures/motion_prompt/sit_prompt.mp4" type="video/mp4">
            </video> -->
            <img src="static/figures/FB15k/FB15k.png" alt="cars peace" width="900" />
            <!-- <video poster="" id="tree" controls width=300>
              <source src="static/figures/motion_prompt/sit.mp4" type="video/mp4">
            </video>
            <p><b>Motion Prompt: “<font color="red">sitting</font>.”</b></p>
            </br> -->
          </div>

        </div>
      </div>
  </section>


  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Research Areas in Knowledge Graphs</h2>
          <div class="content has-text-justified">
            <p>
              Research on knowledge graphs spans four main areas: knowledge representation learning, knowledge
              acquisition, temporal knowledge graphs, and knowledge-aware applications.

              Knowledge representation learning focuses on embedding entities and relations, exploring representation
              space, scoring functions, encoding models, and auxiliary information. Knowledge acquisition addresses
              graph completion, relation extraction, and entity discovery using embedding, reasoning, and learning-based
              methods. Temporal knowledge graphs model evolving facts over time. Knowledge-aware applications integrate
              KGs into tasks like question answering, recommendation, and language understanding.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    </div>
  </section>





  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="column is-centered has-text-centered">
            <img src="static/figures/Taxonomy.png" alt="Taxonomy" width="1200" />
            <!-- <video poster="" id="tree" controls width=500>
              <source src="static/figures/sub_body/video_01.mp4" type="video/mp4">
            </video> -->
          </div>
          <div class="column is-centered has-text-centered">
            <img src="static/figures/GraphEmbeddingTechniques.png" alt="GraphEmbeddingTechniques" width="1200" />
          </div>
          <div class="column is-centered has-text-centered">
            <img src="static/figures/GraphEmbedding.png" alt="GraphEmbedding" width="800" />
          </div>
          <!-- <div class="column is-centered has-text-centered">
            <img src="static/figures/sub_body/3.png" alt="Taxonomy" width="250" />
            <video poster="" id="tree" controls width=500>
              <source src="static/figures/sub_body/video_03.mp4" type="video/mp4">
            </video>
          </div>
          <div class="column is-centered has-text-centered">
            <img src="static/figures/sub_body/4.png" alt="cars peace" width="250" />
            <video poster="" id="tree" controls width=500>
              <source src="static/figures/sub_body/video_04.mp4" type="video/mp4">
            </video>
          </div> -->
        </div>
      </div>
  </section>









  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@misc{hoangminh2025graphcollaborativeattentionnetwork,
      title={Graph Collaborative Attention Network for Link Prediction in Knowledge Graphs}, 
      author={Thanh Hoang-Minh},
      year={2025},
      eprint={2507.03947},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2507.03947}, 
}
    </code></pre>
      <!-- doi          = {10.1145/XXXXXXX.XXXXXXX}, % Replace with actual DOI when available -->
      <!-- articleno    = {XXX},  % Replace XXX with the assigned article number -->
    </div>
  </section>




  <footer class="footer">
    <!--  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
      href="https://homes.cs.washington.edu/~kpar/nerfies/videos/nerfies_paper.pdf">
      <i class="fas fa-file-pdf"></i>
    </a>
    <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
      <i class="fab fa-github"></i>
    </a>
  </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website using template <a href="https://nerfies.github.io/">Nerfies</a> is licensed under a <a
              rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <!-- <p>
            Website source code based on the <a href="https://nerfies.github.io/"> Nerfies</a> project page. If you want
            to reuse their <a href="https://github.com/nerfies/nerfies.github.io">source code</a>, please credit them
            appropriately.
          </p> -->
        </div>
      </div>
    </div>
    </div>
  </footer>


  <script type="text/javascript">
    var sc_project = 12351448;
    var sc_invisible = 1;
    var sc_security = "c676de4f"; 
  </script>
  <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript>
    <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
          class="statcounter" src="https://c.statcounter.com/12351448/0/c676de4f/1/" alt="Web Analytics"></a></div>
  </noscript>
  <!-- End of Statcounter Code -->

</body>

</html>